{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of Kalpani_predict_randomforestclassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalleo1996/pysparkMLExample/blob/main/Copy_of_Copy_of_Copy_of_Kalpani_predict_randomforestclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVgT7Scp99jL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d331cd18-c298-485c-8e44-817b4ec7fcf3"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 12.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=38e99673856d2dbd6973f5071e54217cf31d37fa3b28e88f95f6d14fd3aa1de9\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPuyqaq7Aduc"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEt9UFlABkYl"
      },
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .config(\"spark.driver.memory\", \"15g\") \\\n",
        "    .appName(\"Exploratory Analysis\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqYlTH5BBn77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9f6a9d-10a5-49e1-f2d6-5c9a8e35222f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW35cPY2Bqpd"
      },
      "source": [
        "parking2017 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('/content/gdrive/My Drive/Big Data/Parking_Violations_Issued_-_Fiscal_Year_2017.csv')\n",
        "parking2016 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('/content/gdrive/My Drive/Big Data/Parking_Violations_Issued_-_Fiscal_Year_2016.csv')\n",
        "parking2015 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('/content/gdrive/My Drive/Big Data/Parking_Violations_Issued_-_Fiscal_Year_2015.csv')\n",
        "parking2014 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('/content/gdrive/My Drive/Big Data/Parking_Violations_Issued_-_Fiscal_Year_2014__August_2013___June_2014_.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7VZEwJ1Dqe_"
      },
      "source": [
        "parking2017 = parking2017.dropDuplicates()\n",
        "parking2016 = parking2016.dropDuplicates()\n",
        "parking2015 = parking2015.dropDuplicates()\n",
        "parking2014 = parking2014.dropDuplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_u97Tc8EN__"
      },
      "source": [
        "parking2017 = parking2017.dropna()\n",
        "parking2016 = parking2016.dropna()\n",
        "parking2015 = parking2015.dropna()\n",
        "parking2014 = parking2014.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjvLft2LEjwO"
      },
      "source": [
        "parking2017 = parking2017.toDF(*(c.replace(' ', '_') for c in parking2017.columns))\n",
        "parking2016 = parking2016.toDF(*(c.replace(' ', '_') for c in parking2016.columns))\n",
        "parking2015 = parking2015.toDF(*(c.replace(' ', '_') for c in parking2015.columns))\n",
        "parking2014 = parking2014.toDF(*(c.replace(' ', '_') for c in parking2014.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4HS6LUE4Au"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn5kp3ey4dcT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mPWWkO3FD9l"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEn7pbcNFFqX"
      },
      "source": [
        "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
        "\n",
        "parking2017.createOrReplaceTempView(\"parkingtable2017\")\n",
        "parking2016.createOrReplaceTempView(\"parkingtable2016\")\n",
        "parking2015.createOrReplaceTempView(\"parkingtable2015\")\n",
        "parking2014.createOrReplaceTempView(\"parkingtable2014\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1ZcX0HUwcmz"
      },
      "source": [
        "#parking2017.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRGYxQPdxB8H"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF7Ae3a0IewL"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6R-mMX8MNQv",
        "outputId": "84aa737b-02ce-4214-fe64-f7e62ebb70f2"
      },
      "source": [
        "def get_weekday(date):\n",
        "    import datetime\n",
        "    import calendar\n",
        "    month, day, year = (int(x) for x in date.split('/'))    \n",
        "    weekday = datetime.date(year, month, day)\n",
        "    #return calendar.day_name[weekday.weekday()]\n",
        "    \n",
        "    if calendar.day_name[weekday.weekday()] == 'Sunday':\n",
        "      return 0\n",
        "    if calendar.day_name[weekday.weekday()] == 'Monday':\n",
        "      return 1\n",
        "    if calendar.day_name[weekday.weekday()] == 'Tuesday':\n",
        "      return 2\n",
        "    if calendar.day_name[weekday.weekday()] == 'Wednesday':\n",
        "      return 3\n",
        "    if calendar.day_name[weekday.weekday()] == 'Thursday':\n",
        "      return 4\n",
        "    if calendar.day_name[weekday.weekday()] == 'Friday':\n",
        "      return 5\n",
        "    if calendar.day_name[weekday.weekday()] == 'Saturday':\n",
        "      return 6\n",
        "    if calendar.day_name[weekday.weekday()] == 'Sunday':\n",
        "      return 7\n",
        "\n",
        "spark.udf.register('get_weekday', get_weekday)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.get_weekday>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xtOvGlLakDB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uEcH79Ndieo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Hty912NsM_",
        "outputId": "505d3655-3e06-40d5-93fb-b3df9dc2bc7b"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "\n",
        "def week_number(date_str):\n",
        "    start_day_of_week=7\n",
        "   \n",
        "    def _week_number(date_str):\n",
        "        from datetime import datetime, date\n",
        "        d = datetime.strptime(date_str, '%m/%d/%Y')     # compatible with Python2\n",
        "        wd_d1 = date(d.year, d.month, 1).isoweekday()\n",
        "        offset = (wd_d1 - start_day_of_week + 7 ) % 7\n",
        "        return (d.day - 1 + offset) // 7 + 1\n",
        "    return _week_number(date_str)\n",
        "spark.udf.register('week_number',week_number)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.week_number>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRW9rp-Lb1Yf"
      },
      "source": [
        "from pyspark.sql.functions import udf,desc\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvisW97yI-g6"
      },
      "source": [
        "\n",
        "\n",
        "from pyspark.sql.functions import date_format\n",
        "from pyspark.sql.functions import to_date, date_format\n",
        "\n",
        "from pyspark.sql.functions import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNUHDXsFtB4r"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8laOB89uIBOj"
      },
      "source": [
        "data_2017 = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location, Violation_Code , count(*) as frequency from parkingtable2017  Group by year,month,week,day,Violation_Location, Violation_Code order by year,month,week,day,Violation_Location,Violation_Code \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHJHq01IlAjG",
        "outputId": "46ef71ce-10b1-4956-f67b-1a7e03347040"
      },
      "source": [
        "data_2017.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('year', 'int'),\n",
              " ('month', 'int'),\n",
              " ('week', 'string'),\n",
              " ('day', 'string'),\n",
              " ('Violation_Location', 'int'),\n",
              " ('Violation_Code', 'int'),\n",
              " ('frequency', 'bigint')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bCxe1TsNluf"
      },
      "source": [
        "data_2017.createOrReplaceTempView(\"data_2017\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxTZWDZjN1RL"
      },
      "source": [
        "data_2017 = spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),CAST(Violation_Code AS INTEGER),frequency FROM data_2017\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzTO9c_ZOb0c",
        "outputId": "64ed1537-65f2-43a7-bc0a-d40478c3448b"
      },
      "source": [
        "data_2017.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('year', 'int'),\n",
              " ('month', 'int'),\n",
              " ('week', 'int'),\n",
              " ('day', 'int'),\n",
              " ('Violation_Location', 'int'),\n",
              " ('Violation_Code', 'int'),\n",
              " ('frequency', 'bigint')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD9qXyW8OQDB"
      },
      "source": [
        "data_2017_day = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location,count(*) as totalViolation from parkingtable2017  Group by year,month,week,day,Violation_Location order by year,month,week,day,Violation_Location \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgfT_GUjSqOd"
      },
      "source": [
        "data_2017_day.createOrReplaceTempView(\"data_2017_d\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8yJIp6hSyWa"
      },
      "source": [
        "data_2017_day=spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),totalViolation FROM data_2017_d\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09i5szdxTfR-",
        "outputId": "88f86d36-73fa-45fb-fc86-a26cbe2b46eb"
      },
      "source": [
        "data_2017_day.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('year', 'int'),\n",
              " ('month', 'int'),\n",
              " ('week', 'int'),\n",
              " ('day', 'int'),\n",
              " ('Violation_Location', 'int'),\n",
              " ('totalViolation', 'bigint')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C49zYmmnOdMv"
      },
      "source": [
        "#data_2017_day.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RG98vEOOj0e"
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import round, col\n",
        "df = data_2017.join(data_2017_day, (data_2017.Violation_Location == data_2017_day.Violation_Location) & (data_2017.year == data_2017_day.year)& (data_2017.month == data_2017_day.month)& (data_2017.week == data_2017_day.week) & (data_2017.day == data_2017_day.day))\\\n",
        "    .withColumn(\"violation_probability\", round(F.col(\"frequency\") / F.col(\"totalViolation\"), 5))\\\n",
        "    .drop(data_2017_day.totalViolation).drop(data_2017.frequency).drop(data_2017_day.Violation_Location).drop(data_2017_day.month).drop(data_2017_day.week).drop(data_2017_day.day).drop(data_2017_day.year)\n",
        "#df.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXfRQKR86q-D"
      },
      "source": [
        "for 2014 to 2016 first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oevgkdPw6p5U"
      },
      "source": [
        "data_2014 = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location, Violation_Code , count(*) as frequency from parkingtable2014 where   Group by year,month,week,day,Violation_Location, Violation_Code order by year,month,week,day,Violation_Location,Violation_Code \")\n",
        "data_2014.createOrReplaceTempView(\"data_2014\")\n",
        "data_2014 = spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),CAST(Violation_Code AS INTEGER),frequency FROM data_2014 where year>= 2014 \")\n",
        "\n",
        "data_2015 = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location, Violation_Code , count(*) as frequency from parkingtable2015  Group by year,month,week,day,Violation_Location, Violation_Code order by year,month,week,day,Violation_Location,Violation_Code \")\n",
        "data_2015.createOrReplaceTempView(\"data_2015\")\n",
        "data_2015 = spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),CAST(Violation_Code AS INTEGER),frequency FROM data_2015\")\n",
        "\n",
        "\n",
        "data_2016 = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location, Violation_Code , count(*) as frequency from parkingtable2016  Group by year,month,week,day,Violation_Location, Violation_Code order by year,month,week,day,Violation_Location,Violation_Code \")\n",
        "data_2016.createOrReplaceTempView(\"data_2016\")\n",
        "data_2016 = spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),CAST(Violation_Code AS INTEGER),frequency FROM data_2016\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH0cHnOBUIqv"
      },
      "source": [
        "data_2014.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkUAfpYN458k"
      },
      "source": [
        "data_2014_day = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location,count(*) as totalViolation from parkingtable2014  Group by year,month,week,day,Violation_Location order by year,month,week,day,Violation_Location \")\n",
        "data_2014_day.createOrReplaceTempView(\"data_2014_d\")\n",
        "data_2014_day=spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),totalViolation FROM data_2014_d where year >= 2014\")\n",
        "\n",
        "data_2015_day = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location,count(*) as totalViolation from parkingtable2015  Group by year,month,week,day,Violation_Location order by year,month,week,day,Violation_Location \")\n",
        "data_2015_day.createOrReplaceTempView(\"data_2015_d\")\n",
        "data_2015_day=spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),totalViolation FROM data_2015_d\")\n",
        "\n",
        "data_2016_day = spark.sql(\"select year(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as year,month(from_unixtime(unix_timestamp(Issue_Date, 'MM/dd/yyy'))) as month,week_number(Issue_Date) as week,get_weekday(Issue_Date) as day,Violation_Location,count(*) as totalViolation from parkingtable2016  Group by year,month,week,day,Violation_Location order by year,month,week,day,Violation_Location \")\n",
        "data_2016_day.createOrReplaceTempView(\"data_2016_d\")\n",
        "data_2016_day=spark.sql(\" SELECT CAST(year AS INTEGER),CAST(month AS INTEGER),CAST(week AS INTEGER) , CAST(day AS INTEGER), CAST(Violation_Location AS INTEGER),totalViolation FROM data_2016_d\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSRUxWhXL3I"
      },
      "source": [
        "data_2014_day.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukbLxBiJ7bOW"
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import round, col\n",
        "df_2014 = data_2014.join(data_2014_day, (data_2014.Violation_Location == data_2014_day.Violation_Location) & (data_2014.year == data_2014_day.year)& (data_2014.month == data_2014_day.month)& (data_2014.week == data_2014_day.week) & (data_2014_day.day == data_2014_day.day))\\\n",
        "    .withColumn(\"violation_probability\", round(F.col(\"frequency\") / F.col(\"totalViolation\"), 5))\\\n",
        "    .drop(data_2014_day.totalViolation).drop(data_2014.frequency).drop(data_2014_day.Violation_Location).drop(data_2014_day.month).drop(data_2014_day.week).drop(data_2014_day.day).drop(data_2014_day.year)\n",
        "\n",
        "df_2015 = data_2015.join(data_2015_day, (data_2015.Violation_Location == data_2015_day.Violation_Location) & (data_2015.year == data_2015_day.year)& (data_2015.month == data_2015_day.month)& (data_2015.week == data_2015_day.week) & (data_2015_day.day == data_2015_day.day))\\\n",
        "    .withColumn(\"violation_probability\", round(F.col(\"frequency\") / F.col(\"totalViolation\"), 5))\\\n",
        "    .drop(data_2015_day.totalViolation).drop(data_2015.frequency).drop(data_2015_day.Violation_Location).drop(data_2015_day.month).drop(data_2015_day.week).drop(data_2015_day.day).drop(data_2015_day.year)\n",
        "\n",
        "df_2016 = data_2016.join(data_2016_day, (data_2016.Violation_Location == data_2016_day.Violation_Location) & (data_2016.year == data_2016_day.year)& (data_2016.month == data_2016_day.month)& (data_2016.week == data_2016_day.week) & (data_2016_day.day == data_2016_day.day))\\\n",
        "    .withColumn(\"violation_probability\", round(F.col(\"frequency\") / F.col(\"totalViolation\"), 5))\\\n",
        "    .drop(data_2016_day.totalViolation).drop(data_2016.frequency).drop(data_2016_day.Violation_Location).drop(data_2016_day.month).drop(data_2016_day.week).drop(data_2016_day.day).drop(data_2016_day.year)\n",
        "\n",
        "#df_2015.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agZAcfE1FR_s"
      },
      "source": [
        "#dataframe concatenation\n",
        "from functools import reduce  # For Python 3.x\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "#def unionAll(*dfs):\n",
        "    #return reduce(DataFrame.unionAll, dfs)\n",
        "\n",
        "#full_df = unionAll( df_2015, df_2016,df)\n",
        "#train_df.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "y6nsfvkI_rJ9",
        "outputId": "9c772955-ffec-4eba-8a2b-c0db2c6add83"
      },
      "source": [
        "full_df = df.union(df_2015).union(df_2016).union(df_2014)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8efff8a88b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfull_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_2015\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_2016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_2014\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombinedparkingtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'combinedparkingtable' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rGTwrev0FPmi"
      },
      "source": [
        "#from pyspark.ml.feature import StringIndexer\n",
        "#qualification_indexer = StringIndexer(inputCol=\"day\", outputCol=\"day_chng\")\n",
        "#Fits a model to the input dataset with optional parameters.\n",
        "#df_buck = qualification_indexer.fit(full_df).transform(full_df)\n",
        "#df_buck.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6w3Cik33ARGb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAR8lUwnbzTm"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "#for training data\n",
        "#\n",
        "\n",
        "#day_encoded = OneHotEncoder(inputCol=\"day\", outputCol=\"day_one\")\n",
        "#df_buck = day_encoded.fit(full_df).transform(full_df)\n",
        "\n",
        "#week_encoded = OneHotEncoder(inputCol=\"week\", outputCol=\"week_one\")\n",
        "#df_buck = week_encoded.fit(df_buck).transform(df_buck)\n",
        "\n",
        "month_encoded = OneHotEncoder(inputCol=\"month\", outputCol=\"month_one\")\n",
        "df_buck = month_encoded.fit(full_df).transform(full_df)\n",
        "\n",
        "location_encoded = OneHotEncoder(inputCol=\"Violation_Location\", outputCol=\"Violation_Location_one\")\n",
        "df_buck1 = location_encoded.fit(df_buck).transform(df_buck)\n",
        "\n",
        "code_encoded = OneHotEncoder(inputCol=\"Violation_Code\", outputCol=\"Violation_Code_one\")\n",
        "df_buck2 = code_encoded.fit(df_buck1).transform(df_buck1)\n",
        "\n",
        "\n",
        "#df_buck_train.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za16cr9ZMCu9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T3UTTWtMIUM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyNOvjaCGuv2"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#for training data\n",
        "#assembler = VectorAssembler(inputCols=[\"day_one\",\"month_one\",\"Violation_Location_one\",\"week_one\",\"Violation_Code_one\"], outputCol=\"features\")\n",
        "# Assemble all the features with VectorAssembler\n",
        "required_features = [\"year\",\"month_one\",\"week\",\"day\",\"Violation_Location_one\",\"Violation_Code_one\"]\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "numericCols = [] #['prob']\n",
        "assemblerInputs = [c  for c in required_features] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
        "\n",
        "transformed_data = assembler.transform(df_buck2)\n",
        "#transformed_data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nswy9zAFGkMv"
      },
      "source": [
        "(training_data, test_data) = transformed_data.randomSplit([0.8,0.2], seed =2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yEVDasmLH9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c753aa92-fb59-4869-c31a-6d25f8aceda9"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol='violation_probability')\n",
        "lr_model = lr.fit(training_data)\n",
        "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
        "print(\"Intercept: \" + str(lr_model.intercept))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients: [-0.014050739142411983,0.0,0.07387566576051557,0.03433812606846102,-0.03433086883054008,-0.024713940170639062,-0.01478885755643234,-0.025192982995659464,-0.01026425733246481,-0.04388804569914226,-0.006546651999853586,-0.03936833516020695,-0.005039024228701165,0.010508538613997398,-0.0010711163446267182,0.0,-0.35770741024798003,0.0,0.0,0.0,-0.3661423481260385,-0.24868049814794918,-0.3514731076316914,0.0,-0.3153353960841353,-0.359276646583317,0.0,0.0,-0.33929306089385336,-0.32294420759323234,0.0,0.0,-0.33834310523548705,-0.3583959707397877,-0.3613781769883482,-0.3676263923845598,0.0,0.14549334281331336,-0.32433116777418347,-0.33538767969961947,-0.30133403724711805,-0.2659289219469336,0.0,-0.3468224270936365,0.0,-0.3055198819305495,0.0,-0.329082325007672,-0.29085932974654927,-0.3287834206564618,0.0,0.0,0.0,0.0,0.0,-0.18292776870815572,-0.25870226893789283,-0.2381169164819354,-0.3299920713611229,-0.3560849062298469,-0.3191641745135557,-0.35586301790178543,-0.3211884226120251,-0.3311285814833951,-0.3262560485969035,-0.33332948523111144,0.0,-0.34699205545150985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.32168352911607606,-0.33063852945609973,-0.34828127628608446,-0.3150140128708197,0.0,0.0,-0.2194457256695795,-0.3102928670731317,-0.2322653692757932,-0.2814281097294941,-0.3303521655136243,-0.26852500711257066,-0.3128874690891029,-0.27364868495816425,0.0,-0.33739799710348467,-0.2529994282305067,-0.31175966347834494,-0.18406417140026432,-0.26724300114257554,0.0,-0.016041637902410925,0.0,-0.27519738970690766,-0.3505123780265091,0.0,0.0,0.0,-0.33689756446488245,0.0,-0.22672875625835104,0.0,0.0,0.0,-0.3119143606655275,0.0,0.0,0.0,0.0,0.0,-0.33498520636362444,-0.32368701801618066,-0.3530455333527521,-0.3602203299192706,-0.19442349889845828,-0.321357191121129,-0.3459895417234622,-0.18399425862755273,-0.3038692751977168,-0.341623054090161,-0.3519401259909382,-0.2999057576799464,-0.3243397448960188,-0.31431254482922855,-0.21439785521493565,-0.3579786403669969,0.0,0.0,0.0,0.0,-0.33026579530564515,-0.24465909330077473,-0.19960245592149006,0.0,0.03545675550407386,0.014771776727726267,0.028707909913962406,0.04261276433788185,0.0,0.01693144727081245,0.0,0.027226171354522898,0.05201582241841838,0.029500041482058738,0.01916487978332925,0.02942769036443158,0.0256693193118219,0.17971787831746405,0.0,0.05528829794797005,0.028308779627475935,0.03322322229242654,0.06353444671909107,0.1738111974604721,0.7045156040926058,0.021588841257592327,0.020042380629595794,0.027725416568096923,0.032026024487671646,0.04092862068209033,-0.004381681414737728,0.022183083954278225,0.012669115362201598,0.015922900806941014,0.15232586346775956,0.020574258437895402,0.03705593675939379,-0.0911390501387843,0.03193160587039758,0.0,0.23867148447120437,0.46269110260969804,0.03723993338969904,0.1258263124406826,0.0,0.10547799361191941,0.026966314037505223,0.021752754924155633,0.00731908884706115,0.09932668814847041,0.20338002371908617,0.038911533025883495,0.014408861659929443,0.02190950394187059,0.02117941999922414,0.009140770544825736,0.019815111531084117,0.0,0.011464937529776243,0.0019543857035126434,0.005343611180031602,0.030272215699309074,0.014051343344295997,0.025945371898615278,0.004744951382208339,0.013825479669589504,0.0019024543908900378,0.05332381791725177,0.0032159971768526844,0.021656524563910633,0.008836050528815385,0.019118900829484992,0.2282124779616047,0.07131380705325875,0.17807739357716898,0.016891458302271102,0.005868644908441223,0.02684730035342638,0.003035765650831703,0.0,0.016701703019977265,0.022181982031418884,0.0393433488394624,0.0011072468554412269,0.029423411541243717,0.014347288141635004,0.011441127329278015,0.03371638165404053,0.07851406084231002,0.018353503689420507,0.0,0.0,0.032014645917917965,0.0,0.004691331692982298,-0.013246317282315312,-0.08496984774388655,0.0,0.0,0.004546510042622661,0.031207456562976296]\n",
            "Intercept: 28.611791226113688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRKl6YvyLiKS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "c24bbc95-aa27-444a-9aea-fe5bfee15a83"
      },
      "source": [
        "model_path = \"/content/gdrive/My Drive/Big Data/\" + \"modelday2path\"\n",
        "lr_model.save(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5a77f7cdebbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/gdrive/My Drive/Big Data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"modelday2path\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lr_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT_G-0zyMTR-"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "model2 = LinearRegressionModel.load(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRpiuUb0Mewl"
      },
      "source": [
        "model2_predict = model2.transform(test_data.limit(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1_MInfIMf7Q"
      },
      "source": [
        "model2_predict.select(\"prediction\",\"violation_probability\",\"features\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18hhqS8xMsuq"
      },
      "source": [
        "\n",
        "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
        "print(\"Intercept: \" + str(lr_model.intercept))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4imVQFcMyhK"
      },
      "source": [
        "lr_predictions = lr_model.transform(test_data)\n",
        "lr_predictions.select(\"prediction\",\"violation_probability\",\"features\").show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP0BO1SnM3gO"
      },
      "source": [
        "dataForGrapgh = lr_predictions.select(\"prediction\",\"violation_probability\",\"features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MliDYvrzM9CP"
      },
      "source": [
        "dataG =dataForGrapgh.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwf3nCsGNBOJ"
      },
      "source": [
        "# Necessary imports: \n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn import metrics\n",
        "accuracy = metrics.r2_score(dataG['violation_probability'], dataG['prediction'])\n",
        "print(\"Cross-Predicted Accuracy:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2UXS8zdNCJe"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (24, 8)\n",
        "plt.plot(dataG['prediction'][:100], label = \"Pred\")  # Load the 500 data points from prediction with label name 'Pred'\n",
        "plt.plot(dataG['violation_probability'][:100], label = \"Actual\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}